[ { "title": "Emergence: Memory management", "url": "/posts/EmergenceMemoryManagement/", "categories": "Emergence, Development Log", "tags": "Emergence, C++", "date": "2022-08-09 09:30:00 +0300", "snippet": "It is very important to have consistent memory model for your project from the start, because it is almost impossibleto change it later. In this post I will talk about Emergencememory management library and its design decisions.MotivationHaving the right approach to managing memory is crucial for performance: it impacts both allocation cost andcache coherency. Of course, you can just allocate all the memory on heap by using new, but it will lead to slowallocation, cache misses and memory fragmentation. In my opinion, game engine without consistent memory managementmodel is like giant with legs made of straw. Therefore, I decided that Emergence must be built on top of consistent memory libraryfrom the very beginning. Furthermore, I’ve decided to write my own memory management library to have full controlon how memory allocation works.Writing your own memory management library is not a walk in the park and it’s quite easy to get lost in details.For example, if you’re committed enough, you can try to ignore the existence of C standard library allocationfunctions and write your own platform-specific versions. But it is a tedious and troublesome task. I’ve decidedto be less thorough about that and primarily focus on the API instead of implementation details: once API is doneright and whole project uses it, it shouldn’t be difficult to change implementation details under the hood.One important part about memory management model is that it should not only be consistent and easy to use,but also provide tools for memory usage analysis, for example custom profiler. Having such tools is importantbecause they allow us to visualize how we’re using memory and provide us with insight on what can be improved.When I was searching for such tool to integrate into my memory management library, I’ve mostly encountered toolsthat track memory allocations by file name or by one of the static predefined tags. It looked like it is notgood enough for Emergence: I wanted to be able to builddynamic memory usage flame graph and none of the tools I found was fully capable of doing so. Therefore I’ve decidedthat I need to write my own tool.To summarize, I’ve decided to follow these principles: The most important part is consistent and easy-to-use API. C standard library functions is a good enough backend for the beginning. Library must provide API and a tool for memory usage profiling and analysis.FeaturesIt is important to have a rich enough feature set. It should be neither too small nor too big: it should provideenough control on how memory is allocated, but user must not sink in the sea of details. I’ve decided to go withthis feature set for Emergence memory management library: Unordered pool allocator for lighting-fast allocation and deallocation of objects of the same type. Ordered pool allocator, that has slow deallocation operation, but is more cache coherent and provides iterationover allocated chunks. Stack allocator for lighting-fast cache-coherent allocation of temporary trivial objects. Heap allocator for general purpose allocation. String interning implementation for ASCII strings.It’s better to provide memory usage profiler features as separate list to avoid blending with memory managementfeatures: Memory usage is tracked for logical groups that can be created at any moment during runtime. Memory usage snapshot for all logical groups can be taken at any moment of time. It makes sampling-based profiling possible. Profiler should be able to represent all memory operations as continuous set of events.It makes instrumentation-based profiling possible. There should be an API for serializing and deserializing memory usage tracks: initial memory snapshot and continuous set of events that happened after this snapshot. There should be a tool for viewing memory tracks.Initially I also wanted to implement client-server profiler like Tracy, but I’vedecided to postpone it. It is not so difficult, but it requires time and I’ve decided that it is better to spend this time on other Emergence libraries.AllocatorsPoolPool allocators manage memory in terms of chunks and pages. Chunk is a memory block of predefined size which isusually selected during allocator construction. Page is a large continuous block of chunks. Pages are organized intolinked list: each page contains pointer to the next one. Chunks can either be used or unused: used chunks containuser-defined data and free chunks are organized into free-chunk linked list – every free chunk contains pointerto the next one. To sum up, referencing looks like this:When user requests new chunk from the allocator, we start by checking whether free chunk list is empty. If it’s notempty, we can just pop the first chunk out of this list and return it to user. If there is no free chunks left, weneed to allocate new page and put all the chunks from this page into free chunk list.When user reports that he no longer uses given chunk, we can just put this chunk back into free chunks list and that’sall! But there is one important question: are we putting this chunk into the beginning of free chunk list or into other place? This question defines main algorithmic difference between ordered and unordered pools. Basically, ordered pool guarantees that page list and free chunk list are sorted by addresses in ascending order. You can see that image aboveillustrates ordered pool. This ordering provides several benefits: Memory allocation is more cache coherent as free chunk list ordering minimizes amount of holes between used chunks. Pool shrinking (deallocation of fully unused pages) can be implemented effectively: it can be done in O(pageCount + freeChunkCount) instead of O(pageCount * pageCapacity * freeChunkCount), because we do not needto iterate over all free chunk list to determinate whether chunk is free. Iteration over used chunks can be implemented effectively for the same reason.However, ordering is not a silver bullet, because it has one important flaw: it makes release operation (when user informs that chunk is no longer used) O(freeChunksCount) – we need to find suitable place for new chunkin the list. This can introduce significant performance drops if we’re releasing lots of chunks in a random order.Therefore, usage of ordered or unordered pool is always a trade-off and memory usage strategy should be consideredthoroughly before selecting one pool type over another.Emergence::Memory service providesboth type of pools as separate classes: OrderedPool and UnorderedPool. This allows user to select pool algorithmwith zero runtime overhead. They both provide this set of operations: Acquire requests new chunk from the pool. Release returns chunk that is no longer used to the pool. IsEmpty checks whether pool has any used chunk. Clear deallocates all pages altogether.In addition, OrderedPool supports these operations: Iteration over acquired (used) chunks through BeginAcquired/EndAcquired. Shrink deallocates all pages that have no used chunks.It is worth mentioning that both pools support custom chunk alignment that can be specified during allocator creation.In the end, let’s go over the pros and cons of pool allocator usage.Pros: Generally faster allocation and deallocation: operations on free chunk list are much more lightweight thanoperations on complex heap allocator structures. Protection from memory fragmentation: on the top level allocator works with big memory blocks (pages) insteadof small blocks and that reduces risk of memory fragmentation by a lot. Cache coherency: chunks are allocated on continuous blocks of memory (pages), therefore in most cases logicallyadjacent data will be stored in adjacent memory addresses.Cons: You need to manually shrink/clear pool allocators if your memory usage strategy is not stable. For example,you may allocate lots of chunks for temporary objects during level loading and you will no longer need this dataafter level is loaded, so you need to manually clear or shrink that pool. Pool allocators only work for fixed size chunks with size greater or equal to the size of pointer. If you needvariable-size cache coherent allocations or need to allocate smaller blocks of memory you need to use other approach.StackStack allocators usually operate on preallocated block of memory and provide memory in a stack-like fashion byincreasing stack top pointer. At the beginning stack top pointer looks at first byte of allocator-managed memoryregion. Each allocation is done by moving stack top pointer forward. Deallocations are done by moving this pointerbackward. You can already see the problem in this pattern: deallocation can not be done in random order. That’s true:there is no canonical deallocation in stack allocator API, instead there is return-to-checkpoint operation, that movesstack top pointer to saved position from the past. This operation deallocates everything that was allocated aftercheckpoint.Emergence::Memory service providesimplementation of this type of allocator through Stack class. It preallocates memory block of given capacity and operates on top of this block. Stack supports following operations: Acquire – allocates given amount of bytes with given alignment and moves stack top forward. Head – returns stack top pointer that can be saved and used as checkpoint later. Release – moves stack top pointer back to given checkpoint. Clear – moves stack top pointer back to the beginning. GetFreeSpace – returns how much space is left to be used.Stack allocators are simple and powerful, but also usually a niche solution. Let’s go over its pros and cons.Pros: Lighting-fast allocation: nothing can be faster than one simple pointer operation. Lighting-fast mass-deallocation: we can deallocate all object in one pointer operation without updatinganything except stack top pointer. Does not cause memory fragmentation: stack only preallocates memory during construction and never actuallyallocates memory from heap after that.Cons: Stack allocator is only suitable for trivially-destructible objects due to its mass-deallocation approach. Stack operates on fixed amount of memory and can never grow bigger than predefined capacity. Stack does not support selective deallocation: you either deallocate everything by reverting to a checkpoint ordeallocate nothing.To sum up, stack allocator is one of the best examples of niche solutions. It is almost completely unusable forgeneral purpose situations, but is very helpful for some special cases. For example, it is a usual solution fortemporary trivial objects with one-frame lifetime: during frame all systems than need such objects request memoryfor them though stack allocator and when frame ends these objects are easily deallocated all-at-once.HeapHeap allocators theme is quite interesting from technological point of view and it deserves a separate article for sure!But in Emergence I’ve decided to avoid inventing my own bicycle,which is quite rare as I’m almost always inventing my own bicycles, and use malloc, realloc and free under thehood of Heap wrapper-class. Heap allocators are universal generic solution, but there are reasons for the existenceof other allocators, so let’s go over pros and cons of this allocator type.Pros: Universal: allocate whatever you want whenever you want.Cons: Slower than specialized allocators, like pool and stack allocators. Has high risk of memory fragmentation when lots of small objects are allocated and deallocated.String interningString interning is an important technique that allows to both save the memory and improve application performanceif you use it right. Interned strings are stored in a special pool and are never duplicated. That makes equality check,hash calculation and string copying lighting fast, because interned string object is essentially just a pointerto the real string in interned strings pool. But it also slows down string creation from raw values, because it addsstring interning pass to construction routine.Emergence::Memory service providesstring interning implementation for ASCII strings through UniqueString class. Let’s take a quick look at how interned strings pool is organized there.Instead of spreading strings all over the memory (which would cause memory fragmentation), we’re storing them inspecial memory structure: pool of string stacks. It is essentially pool allocator of stack allocators that are usedto allocate interned string values. When current string stack is filled we allocate new stack and continue to allocatestrings through it. Interned strings are never deallocated as it makes interning logic more performant and easy-to-use.To check whether string value is already interned or not we keep separate hash set of strings that hashes internedstrings by their values instead of their pointers. It allows us to do this check in almost O(stringLength), but, of course, eats additional memory.As every other memory-related solution, string interning is not a silver bullet and has its pros and cons.Pros: Provides lighting-fast equality check, hash calculation and string copying. Guarantees that strings are not duplicated in memory.Cons: Most implementations store interned values for whole program execution, therefore temporary strings should neverbe interned. Interning is not a fast operation, so it should not be called every frame and should not be called on temporarystrings.Implementing your own string interning library is much easier than it sounds! If you wanna try, check out my article about implementation of trivial string interning routine.ProfilingIt’s important to understand how your memory model scales as the project growth and good memory profiler is crucialto achieve this goal. Good memory profiler provides developers with tools that allow to see the big picture and to identify problems early. I’ve decided that I need following features from Emergence memory profiling: It should be instrumental, but still reasonably fast and lightweight. Instrumental profiling would allow us to trackevery memory operation and find even the smallest bad patterns like continuous allocation and deallocation in oneparticular task every frame. It should be able to differentiate between used and reserved memory. Emergence uses pool allocators a lot so it is very importantto see whether memory is used or reserved for future usage. It should be able to provide memory usage flame graph at any particular moment of time. Flame graphs are quite useful,because they allow to easily switch from bigger picture to details and vice versa. For example, it could be easy to see that particular component storage consumes too much memory, because it is too wide. Flame graph allows us to gofurther into to see what actually consumes memory: for example, component data or indices. Image below shows that indices of StaticModelComponent consume half of the storage memory! It should provide an API for implicit hierarchy creation. For example, let’s imagine that we have a Storage libraryand a StorageManager library. Storage knowns nothing of StorageManager, but it should be a child of StorageManager on flame graph. So, there should be a way to pass this hierarchical dependency to Storageimplicitly, without explicitly adding knowledge of StorageManager.I didn’t find any ready-to-integrate solution that has all these features at once, therefore I’ve decided to writemy own memory profiling solution and integrate it with Emergence::Memory allocators.Also, I’ve chosen to separate this solution into 3 parts: profiling backend, that is directly integrated withEmergence::Memory service,runtime tracking library with serialization support and client application. I’ll describe each part separately.Profiling backendEmergence::MemoryProfiler serviceis used as profiling backend: it implements memory usage calculations, registers operations and provides low-level capture API. It was intentionally separated from Emergence::Memory service fortwo reasons: We need to be able to disable memory profiling in release builds by selecting special empty implementation and thatshould not affect memory allocators implementation selection. Profiling logic is not technically coupled with allocation logic, so there is no sense to introduce couplingby merging them into one service.The heart of the profiling backend is AllocationGroup class which provides API for memory operations registration.AllocationGroup is not only a registration API provider, but also is a part of memory usage hierarchy: groupsare organized into tree graph with predefined Root group as tree root. Every AllocationGroup stores its id,reserved memory amount, used memory amount and pointers to parent, first child and next on level groups. Although referencing model might look a bit weird, it allows us to avoid usage of complex containers. Picture below illustrates how AllocationGroup referencing looks like.AllocationGroups themselves are allocated through unprofiled OrderedPool. Of course, it is impossible to profile how much memory profiling takes, because it would introduce cyclic dependency in initialization order. Usage of OrderedPool helps us to improve performance of AllocationGroups by making access to distinct groups cachecoherent.One of the important questions that I asked myself was: how to make AllocationGroup construction and management as convenient as possible? I’ve came up with two fundamental principles: AllocationGroup class works as handle to real allocation group, therefore AllocationGroup construction does notalways results in construction of an actual allocation group. If allocation group with the same id already exists,new AllocationGroup will simply reference this implementation-level allocation group. This approach allows user tocreate AllocationGroup instances whenever he needs without worrying about referencing and duplicates: everythingis resolved on implementation level. Thread-local AllocationGroup stacks should be used to provide implicit parent selection unless parent group isselected explicitly. Stacks fit perfectly into this task: they provide intuitive interface for tasks like this.Also, stack usage allows to create connection between allocation groups from different modules without actuallyadding knowledge about each other to these modules. For example:// Storage module places its own allocation group on top before construction an object.auto placeholder = GetAllocationGroup ().PlaceOnTop ();objectMapping.Construct (object);// Object expects right group to be placed on top during construction.class MyCustomObject final{ // ... Container::Vector&lt;StandardLayout::Patch&gt; patches {Memory::Profiler::AllocationGroup::Top ()}; // ...};Now let’s get a quick look at AllocationGroup registration methods: Allocate registers increase of reserved memory amount. Acquire registers transfer from reserved memory into used memory. Release registers transfer from used memory to reserved memory. Free registers decrease of reserved memory amount.As you can see, this API is tailored for allocators than deal with memory reservation, which is the case for mostallocations inside Emergence.Now it’s time to have a look at low level capture API. It consists of two major parts: CapturedAllocationGroup class, that represents state of specific allocation group at the moment when capturewas started. User receives CapturedAllocationGroup of predefined root AllocationGroup that representscaptured allocation group hierarchy and can be traversed like normal allocation group hierarchy. Event model, that consists of Event structure, that contains all the info about one specific memory operation,and EventObserver, that provides API for reading new events and works as a mailbox: user can extract eventsin historical order at any moment. One important implementation detail is that all observers use shared eventqueue which makes event management complexity independent of observer count. To start capturing memory profiling data user just needs to call Capture::Start method, that will captureall AllocationGroups and create EventObserver for observing everything that happened after capture. To makeevent sequence easier to analyze, special marker events are also supported: user can put markers with custom idsusing AddMarker method. Markers are treated like usual memory operation events. Although this API is pretty simple and straightforward, it is powerful enough to be foundation for more complex and sophisticated top-level API.One significant theme, that I missed above, is how profiling backend deals with multithreading. I’ve though abouttrying to use some elaborate solution for this, but in the end I’ve decided to use one shared spin lock for allprofiling-related operations. And it’s not as clumsy as it may sound: all profiling methods are already tailoredto be as quick and small as possible, therefore lightweight spin lock looks like a perfect fit here.Runtime tracking with serialization supportProfiling backend provides API for retrieving data, but retrieving is not enough: we need a library that managesprofiling data and provides tools to analyze and serialize this data. That’s the goal ofEmergence::MemoryRecording library.Its API provides user with ability to: Examine state of any allocation group at any moment of time provided this moment is inside profiling track. Read all events inside profiling track in historical order. Serialize and deserialize profiling tracks. Capture profiling tracks at runtime.The most important part of Emergence::MemoryRecording libraryis Track class, that stores full profiling track data and provides an API to iterate over that data and fetchallocation group states. That means that Track plays both the role of data storage and data provider.As data storage, Track stores all profiling events in linked list, backed by pool allocator. For algorithmic convenience initial state of captured groups is also provided as group declaration events. We need this type of events to store information data about newly found allocation groups, otherwise we would need to duplicate this information inevery operation event. So, if this additional event type is already needed, why not use it to save initial state too?As data provider, Track provides state of any allocation group through RecordedAllocationGroup class instances. This class mimics CapturedAllocationGroup, but its instances are managed and update by owner Track. Time selectionis done through movement of current event iterator using MoveToPreviousEvent and MoveToNextEvent methods. This approach is more universal than direct selection of time, because it allows iteration over all events withoutworrying about how they are separated by time. Also, it allows user to user to observe state of memory after anyoperation even if two operations are separated by very small amounts of time like 1 microsecond.But how to populate Track with profiling data? There are two ways to do it: capture data at runtime usingRuntimeReporter or load data from stream using StreamDeserializer. RuntimeReporter session is initializedthrough Begin method that accepts Track pointer and reference to captured root group, that will be convertedinto group declaration events for whole hierarchy. Then it expects user to pass events from EventObserver toRuntimeReporter through ReportEvent method whenever user wants to update Track. Session could be ended throughthe End method and after that RuntimeReporter could be reused. StreamDeserializer follows the session pattern: it requests track and stream pointers in Begin and parses events one-by-one using TryReadNextEvent method. One-by-one parsing is especially important when profiling tracks are quite long: it allows to spread data loading to several frames and by doing so avoid freezing tool that loads this profiling track. And due to instrumental profiling tracks usually grow quite fast, so it is important to avoid reading everything at once.Serialization is done through StreamSerializer, that kind of mimics RuntimeReporter, but uses output stream astarget instead of Track instance. Its Begin method accepts pointer to output stream and reference to capturedroot group. Then serialization is done using SerializeEvent method that takes profiling event as parameter andcreates group declaration event for profiling track automatically if it is needed. One-by-one serialization isused for flexibility: user can limit how much time is spent serializing profiling data.To sum up,Emergence::MemoryRecordingprovides high-level API for working with profiling data, including state playback and serialization.Client applicationI’ve created a client applicationfor viewing serialized profiling tracks: It shows memory usage flame graph with every group with used-to-total memory percentage. Flame graph visual can be scaled and moved in any way user likes. It shows timeline with markers. Markers with high frequency are shown only when timeline scale is low enough. It shows list of events that are near current event, which allows user to easily jump to required event. It allows user to select group on flame graph and view detailed info about this group and its children.Client application uses ImGui and SDL2. It is a thinUI layer built on top of Emergence::MemoryRecording,therefore it’s not really a lot to discuss here.In future I’m planning to add automatic analyzers to client application that will track down common memory usage errors,that lead to performance drops. For example, underreservation, when there is not enough memory reserved for particulargroup and it always allocates new memory and them frees it, and overreservation, when too much memory is reserved forparticular allocation group and this memory is never used to full extent.ConclusionAs I said earlier, having consistent memory model and using a right approach for memory management in project isvery important. That’s why I’ve spent a lot of time on designing and testing Emergence memory-related libraries. I cannot say that theselibraries are ideal as nothing is ideal in this world, but I believe that they’re good enough to support this projectand its growth.Hope you’ve enjoyed reading! If you have any suggestions, feel free to contact me through telegram or email." }, { "title": "Emergence: Reflection", "url": "/posts/EmergenceReflection/", "categories": "Emergence, Development Log", "tags": "Emergence, C++", "date": "2022-07-23 13:50:00 +0300", "snippet": "Reflection system is an important part of every engine, but there is no standard solution for that in C++: there arelots of libraries with their pros and cons. I’ve decided to write my own reflection for Emergence and I will describe it in this post.MotivationIt’s impossible to underestimate how useful good reflection system could be: reflection provides wide variety oftools to make code more flexible, robust, elegant and give it some kind of consciousness. But everything comeswith a price: reflection might consume a lot of compile time or be rather slow from performance point of view.Therefore I’ve decided to write my own reflection system that is fine-tuned forEmergence project needs and is as lightweight as possible.When you’re starting to design your own solution it is crucial to define usage scope and examine expected use cases.Otherwise, it is easy to build complex orbital gun with lots of unusable features. For the Emergence main usage scope of reflection is managing data:observing field values, changing them or iterating through the whole data structure. There are some examples: Indices observe specified fields and process their values changes. Celerity events use reflection to extract and copy useful data from objects. Serialization library, which is not finished yet, uses reflection to iterate over data structure to serialize or deserialize it. Reflection is used as a high-level parameter for generic pipeline tasks likeCelerity::Assembly extension.After examination of these use cases I’ve decided that: There is no need to reflect methods: reflecting fields, no-argument constructor and destructor is enough. We can focus on standard layout types and ignore other types because in the most cases data is stored in such types. Reflection must provide API for O(1) field access: no search, no nested hierarchy traversing. Reflection headers must be lightweight and template-free for optimal compile time. Reflection must be aware of unions and inplace vectors: otherwise iteration would show irrelevant fields.Thats how the idea of StandardLayoutMapping service was born.Features overviewBefore diving into details it is good to inform the reader what my reflection system can and cannot achieve. Let’s start from the features: Lightweight field-only reflection for standard layout types. Field identifiers and handles provide O(1) access to field info. Conditional field iteration is aware of unions and inplace vectors in the context of given data object. One-bit flags are supported. Field identifiers are stable and can be used for serialization. Patches provide generic way to apply changes to an object.But there are some limitations too: Only standard layout types are supported. Method registration is not supported. Field meta registration is not supported.I would like to add my personal opinion about field meta, because it’s not so difficult to implement, but neverthelessI’ve decided to omit this feature. The thing is that usually meta adds unnecessary coupling between data and its usage.It looks very convenient and useful at the first glance, but than you might end up with transform implementation thatdepends on networking implementation in order to add networking-related meta. Or all meta types might be declaredas one big heap and included everywhere. But that is not the biggest problem: what if you need different networkingor interpolation or some other meta-based settings for shared data type in two different projects? For example,one project needs to replicate scale and the other one doesn’t. So I’ve decided to avoid using meta at all in orderto prevent such problems from appearing.Terms Mapping – structure than contains all the info about registered data type. MappingBuilder – special object that provides API for Mapping creation. Field – handle to the information about one concrete field, belongs to Mapping. FieldId – special id that is unique in context of Mapping and allows to get Field in O(1). Patch – archive of changes, linked to Mapping, that can be applied to an arbitrary object. PatchBuilder – special object that provides API for Patch creation.Registering typesLet’s explore how type registration works in Emergence.Actually, there is two registration APIs: verbose core API that is represented by MappingBuilder class andmore user-friendly macro-based approach from MappingRegistration header. The first one is a basic API that allows user to register types whatever way user wants. The second one is built on top of the first one andrepresents the approach Emergence uses to register types.Choice to have both the verbose basic approach and the simplified macro approach was made in order to achieveflexibility: if user is satisfied with macro-based approach, he can use this approach, otherwise it ispossible to implement custom registration approach which will still produce compatible results.MappingBuilder is essentially an implementation of classic Builder pattern for Mapping: it encapsulatesregistration process and ensures that all Mapping instances are finished and ready to use. Registration throughMappingBuilder API is quite simple, but verbose. It looks like this:MappingBuilder builder.builder.Begin (\"MyComponent\"_us, sizeof (MyComponent), alignof (MyComponent));builder.SetConstructor (...);builder.SetDestructor (...);// ...FieldId fieldA = builder.RegisterInt16 (\"a\"_us, offsetof (MyComponent, a));FieldId fieldB = builder.RegisterFloat (\"b\"_us, offsetof (MyComponent, b));FieldId fieldC = builder.RegisterNestedObject (\"c\"_us, offsetof (MyComponent, c), typeCMapping);// ...Mapping typeMyComponentMapping = builder.End ();As you can see, this approach only cares about putting fields into Mapping, but not about storing Mappings andFieldIds. Also, it forces us to duplicate field names and types. Obviously, this API wasn’t designed for directuse – it was designed to be the simplest base API for building custom better APIs. And Emergence offers such API out of the box in MappingRegistration header.In MappingRegistration approach every type stores information about itself in special Reflection structureand provides instance of this structure through static Reflect method, like this:struct MyComponent final{ // Fields of our component. int16_t a = 0; float b = 0.0f; SomeClass c; // Reflection structure that lists all reflected // information about this type. struct Reflection final { FieldId a; FieldId b; FieldId c; Mapping mapping; }; // Static method for getting reflection info. static const Reflection &amp;Reflect () noexcept;};Reflection structure defines how we store reflection data. We still need to duplicate field names: it’s far from ideal, but I wasn’t able to come up with any solution that avoids it entirely. The best thing about this technique of storing reflection data is that it provides universal form of access to this data making registration very easy:const MyComponent::Reflection &amp;MyComponent::Reflect () noexcept{ static Reflection reflection = [] () { EMERGENCE_MAPPING_REGISTRATION_BEGIN (MyComponent); EMERGENCE_MAPPING_REGISTER_REGULAR (a); EMERGENCE_MAPPING_REGISTER_REGULAR (b); EMERGENCE_MAPPING_REGISTER_REGULAR (c); EMERGENCE_MAPPING_REGISTRATION_END (); }(); return reflection;}As you can see, everything about registered fields is deduced automatically and we’re not duplicating anything now!It’s neat, isn’t it?You can also notice that macro is named EMERGENCE_MAPPING_REGISTER_REGULAR instead of just EMERGENCE_MAPPING_REGISTER. It is called like that because we have two “irregular” field archetypes: inplacestrings and plain blocks of memory. Right now we can not safely deduce whether given field is inplace string orplain block, therefore we’re registering them though separate macros: EMERGENCE_MAPPING_REGISTER_BLOCK andEMERGENCE_MAPPING_REGISTER_STRING.But what about arrays? Thankfully, there is solution for that too!// Header.struct ArrayComponent final{ std::array&lt;uint32_t, 32u&gt; ints; struct Reflection final { // We declare array of fields inside our reflection structure. std::array&lt;FieldId, 32u&gt; ints; Mapping mapping; }; static const Reflection &amp;Reflect () noexcept;};// Object.const ArrayComponent::Reflection &amp;ArrayComponent::Reflect () noexcept{ static Reflection reflection = [] () { EMERGENCE_MAPPING_REGISTRATION_BEGIN (ArrayComponent); EMERGENCE_MAPPING_REGISTER_REGULAR_ARRAY (ints); EMERGENCE_MAPPING_REGISTRATION_END (); }(); return reflection;}Field projectionImagine that we have structure that contains other structures as fields:struct Inner final{ float x = 0.0f; float y = 0.0f; // ... Reflection ...};struct Complex final{ Inner a; Inner b; Inner c; // ... Reflection ...};Accessing field x of Inner by FieldId in O(1) looks trivial: we can just store fields in vector. The same goesfor field b of Complex. But what about field x of field c of Complex? We still want to access it in O(1),but there is no trivial way to do it.Technique to solve this issue is called field projection: if x is field of a and a is field of n, thena.x is a field of n too. Therefore, our Complex structure has whole bunch of fields after projection:a, a.x, a.y, b, b.x, b.y, c, c.x and c.y. In order to make this technique mathematically completewe also need to define projection function: FieldId Project (FieldId rootObjectField, FieldId nestedObjectField).For example, FieldId of a.x is equal to Project (Complex::Reflect ().a, Inner::Reflect ().x). As these projectedids are stable we can safely cache them and pass as parameters whenever we need to. In Emergence this function isimplemented as Emergence::StandardLayout::ProjectNestedField and is actually just a sum of rootObjectField and nestedObjectField! We are able to make it that simple because we are doing projecting right after structurefield registration.Like any other solution, field projection technique has its pros and cons.Pros: It provides O(1) access to any field of standard layout type. It makes algorithms that make use of linearized structure data, for example binary serialization, much easier to implement, because projection automatically makes reflection data linear.Cons: It uses lots of memory because of field info duplication: only offsets and ids are changed during projection process,but we need to duplicate whole infos. It makes algorithms that make use of tree-like structure data, for example YAML serialization, a bit more difficultto implement, because they need to skip projected fields everywhere.For Emergence, O(1) access to any field was the main reasonto stick to this technique: reflection data is used extensively by storage management logic, for example record indexing, therefore reflection access speed defines how effective storage management is.Conditional field iterationIt is important to provide user with API that allows to iterate over contextually relevant fields. For example,let’s take a look at that structure:struct CollisionGeometry final{ CollisionGeometryType type; union { Math::Vector3f boxHalfExtents; float sphereRadius; struct { float capsuleRadius; float capsuleHalfHeight; }; }; // ... Reflection ...};Technically it has 5 fields, but not more than 3 fields are contextually relevant at the same time becauseall fields except one are inside union. For example, for spheres only type and sphereRadius fields are relevant. If we’re using reflection to serialize object or log it somewhere, we need to skip these irrelevant fields.The same thing is true for inplace vectors: if vector can hold up to 6 elements, but holds only 2 right now, we shouldnot iterate over garbage memory, stored in last 4 elements.At first, I though that unions and inplace vectors are different cases and should be handled in a different way, but then I came up with the idea of conditional field iteration: union switch value or count of elements is just anargument to conditional expression that decides whether field is visible or not in the current context. And there isno need to waste memory by attaching condition to every field: we only need to specify intervals where conditionsare active. Also, it makes sense to organize conditions as a stack: we are generally adding and removing them whileregistering fields.It all might sound too abstract and high level, so let’s switch to actual examples. This is registration functionfor our CollisionGeometry:const CollisionGeometry::Reflection &amp;CollisionGeometry::Reflect () noexcept{ static Reflection reflection = [] () { EMERGENCE_MAPPING_REGISTRATION_BEGIN (CollisionGeometry); EMERGENCE_MAPPING_REGISTER_REGULAR (type); EMERGENCE_MAPPING_UNION_VARIANT_BEGIN (type, 0u); EMERGENCE_MAPPING_REGISTER_REGULAR (boxHalfExtents); EMERGENCE_MAPPING_UNION_VARIANT_END (); EMERGENCE_MAPPING_UNION_VARIANT_BEGIN (type, 1u); EMERGENCE_MAPPING_REGISTER_REGULAR (sphereRadius); EMERGENCE_MAPPING_UNION_VARIANT_END (); EMERGENCE_MAPPING_UNION_VARIANT_BEGIN (type, 2u); EMERGENCE_MAPPING_REGISTER_REGULAR (capsuleRadius); EMERGENCE_MAPPING_REGISTER_REGULAR (capsuleHalfHeight); EMERGENCE_MAPPING_UNION_VARIANT_END (); EMERGENCE_MAPPING_REGISTRATION_END (); }(); return reflection;}As you can see, there is nothing about conditions yet: MappingRegistration hides them under macros for readability.But what is hidden under EMERGENCE_MAPPING_UNION_VARIANT_BEGIN and EMERGENCE_MAPPING_UNION_VARIANT_END macros?#define EMERGENCE_MAPPING_UNION_VARIANT_BEGIN(_selectorField, _switchValue) \\ builder.PushVisibilityCondition (reflectionData._selectorField, \\ Emergence::StandardLayout::ConditionalOperation::EQUAL, _switchValue)#define EMERGENCE_MAPPING_UNION_VARIANT_END() builder.PopVisibilityCondition ()These macros are just operating with conditions through MappingBuilder interface. When union begins, we’re pushingcondition that says: fields below are visible only when _selectorField is equal to _switchValue. And when unionends we’re just popping this condition out. There are also other conditional operations, for example inplace vector registration makes use of &gt; (see EMERGENCE_MAPPING_REGISTER_REGULAR_VECTOR macro):builder.PushVisibilityCondition (_sizeField, ConditionalOperation::GREATER, index);This condition says that fields below should be visible only when vector size aka _sizeField is greater than elementindex. Just like that: nothing less, nothing more. Simplicity of this technique makes it very flexible: it is notjust for unions and inplace vectors, it can be used anywhere if user needs it.Of course, conditional iteration is less performance-friendly that plain iteration, therefore Mapping hastwo iteration options: Begin/End for non-conditional iteration and BeginConditional/EndConditional forconditional iteration. You can use conditional iteration just like the usual one:for (auto iterator = _mapping.BeginConditional (_object), end = _mapping.EndConditional (); iterator != end; ++iterator){ StandardLayout::Field field = *iterator; /// ...}There is one important thing about conditional iteration performance: you might think that it is very slow due tocondition stack operations – stack push/pops, memory allocation for that and so on. But it is actually not true!Because push/pop order is always the same, we can get rid of stack operations during conditional iteration by baking the operations during type registration. I will not dive into details of this algorithm here, but keep inmind: conditional iteration is not as slow as you might think.PatchesSometimes it is useful to store difference between two possible states of an object. For example, it can be usedfor prefab system to apply prefab values to freshly constructed objects. Emergence supports this through patches feature: at any momentuser can create Patch using PatchBuilder and then apply this Patch whenever it is needed.There is two ways to create a patch. First one is to manually list all the differences like that:PatchBuilder builder;builder.Begin (Player::Reflect ().mapping);builder.SetBit (Player::Reflect ().alive, false);builder.SetBit (Player::Reflect ().poisoned, true);Patch patch = builder.End ();The second one is much simpler: it automatically creates patch from difference between two objects. For example:Player initial;Player other = initial;other.flags = (1u &lt;&lt; Player::ALIVE_FLAG_OFFSET) | (1u &lt;&lt; Player::POISONED_FLAG_OFFSET);Patch patch = PatchBuilder::FromDifference (Player::Reflect ().mapping, &amp;other, &amp;initial);For performance reasons Patch system has one limitation: it works only with fields that occupy 8 or less bytes ofmemory, so inplace strings and inplace blocks of memory are ignored. But not unique strings, inplace vectors and nested objects! Thanks to field projection technique inplace vectors and nested objects are analyzed as sets ofdisconnected fields and therefore are freely processed by Patches.After creation Patches can be easily applied to any object of target type:patch.Apply (&amp;object);Just like Mappings, Patches can be moved, copied and stored anywhere you need.Implementation detailsEmergence reflection system was implemented with memory usageand cache coherency in mind. All internal data is stored as close to each other as possible: Mapping with fields is one continuous block of memory: it is reallocated during creation process to ensure thatit uses just enough memory to store all the fields and not more. This allows field iteration to be as cache coherentas possible and also makes simultaneous access to multiple fields cache coherent too. Field visibility conditions are stored in special pool in order not to interfere with the field data. In the meantime,pool makes sure that conditions are laid down continuously unless page ends. Therefore condition access during conditional allocation is also cache coherent. Like Mapping, Patch is also represented by one continuous block of memory, that is reallocated during creation process. It makes patch application process as cache coherent as possible.Also, both Mapping and Patch are actually managed as resource handles, therefore copying a Mapping or a Patchdoes not result in actual data duplication: it only increases resource reference count.Reflection usageStandardLayoutMapping service is a backbone of Emergence and used almost everywhere. There isa quick summary of what it is used for: It powers the RecordCollection serviceby allowing it to use any record field for any index that needs it. Reflection also provides constructors and destructors for the records. It powers the Warehouse serviceby providing enough information for prepared query and object storages creation. It provides Celerity librarywith information for query preparation, event management and pipeline validation. Patches are the backbone of Celerity::Assembly library: they are used to initialize freshly created objects with required data.To summarize, StandardLayoutMapping serviceis a very important part of Emergence project that powers lotsof other high level libraries. It was specially designed and optimized for optimal usage inside these libraries.Hope you’ve enjoyed reading! If you have any suggestions, feel free to contact me through telegram or email." }, { "title": "Tutorial: String interning", "url": "/posts/TutorialStringInterning/", "categories": "Tutorials, Memory", "tags": "Tutorials, C++", "date": "2022-07-12 21:30:00 +0300", "snippet": "Is there a lot of instances of the same string in your application that are wasting lots of space? Or are you checking equality or hashing strings quite often and wondering how to do it more efficiently?Fortunately, string interning comes to the rescue!MotivationLet’s start from describing two most common cases where string interning helps us a lot.Strings are much more informative for humans than numbers. It is easy to understand what we’re dealing with whenwe’re seeing unitType = \"Knight\", but it is much harder if we’re seeing unitType = 42. Therefore string isthe best variant when we need to refer to a well-known object like config entry. But it also means that suchstring will be copied lots of times! What if every unit has std::string unitType and there is 100 units? Wewould waste 99 * X bytes where X is the average config entry id length. And what if there is more units?Not looking good, eh?Checking string equality is much less appealing than checking number equality – it’s O(string length) insteadof O(1) after all. The same applies to string hashing – it is also O(string length). So if we are checkingstring equality or hashing strings a lot, we might end up with a bottleneck: string iteration would consume much more time than actual algorithm. But there should be a way to avoid this problem, right?Now it is the time to unravel the mystery of string interning: all interned strings are immutable and are stored in a special storage that contains not more than 1 instance of a string. So interned strings are never duplicated and therefore don’t waste memory! But that’s not all: if string is never duplicated then we’re free to use pointer equality check instead of string equality check and by that we can check if strings are equal in O(1). The same goes for hashing: why not just use unique string pointer as hash?Simple implementationTo get better understanding of string interning concept we will start from simple implementation: We will ignore string encoding and just use chars. We will ignore anti-defragmenetation tecnique. We will ignore unused string deallocation. We will ignore multithreaded access.Now let’s examine our interned string class:// InternedString class instances share immutable string// that is allocated inside string pool.class InternedString final{public: InternedString () = default; // Let's accept both c-style strings and modern string views // as constructor arguments. explicit InternedString (const char *_string) noexcept; explicit InternedString (const std::string_view &amp;_string) noexcept; // Dereference operator will return pointer to actual string. const char *operator* () const noexcept; // Interned string hash that is computed in O(1). [[nodiscard]] uintptr_t Hash () const noexcept; // Interned string equality check that is done in O(1). bool operator== (const InternedString &amp;_other) const; bool operator!= (const InternedString &amp;_other) const;private: // Pointer to location of actual string in memory that // might be shared between many InternedString instances. const char *value = nullptr;};InternedString interface is quite simple and straighforward, isn’t it? Let’s switch to implementation:#include &lt;cassert&gt;#include &lt;cstring&gt;#include &lt;unordered_set&gt;#include \"InternedString.hpp\"// Registrar function is the most important part in any string// interning implementation. This function checks whether given// value is already interned and interns it if it's not.//// There are 2 common questions that are answered by registrar function:// - How value lookup is implemented?// - How interned values are allocated?//// Answers to these questions describe strengths and weaknesses of// registrar function.static const char *RegisterValue (const std::string_view &amp;_value){ // There is no sense to allocate space for empty strings. if (_value.empty ()) { return nullptr; } // In this simple implementation we use unordered set to // store views of all interned values. It is a trivial // solution, but it is quite common nevertheless. static std::unordered_set&lt;std::string_view&gt; stringRegister; // Lets check whether value is already interned. auto iterator = stringRegister.find (_value); if (iterator == stringRegister.end ()) { // Value is not interned: allocate memory and insert it. char *space = new char[_value.size() + 1u]; // We need to add 1 byte for null-terminator. strcpy (space, _value.data ()); auto [insertionIterator, result] = stringRegister.emplace (space); assert (result); return insertionIterator-&gt;data (); } // Value is already interned: we can just return pointer to it. return iterator-&gt;data ();}// In constructors, we're getting pointers to interned values (and interning this value if needed).InternedString::InternedString (const char *_string) noexcept : InternedString (std::string_view {_string}){}InternedString::InternedString (const std::string_view &amp;_string) noexcept : value (RegisterValue (_string)){}const char *InternedString::operator* () const noexcept{ return value;}uintptr_t InternedString::Hash () const noexcept{ // We can use pointer as hash result, // because interned strings are never duplicated in memory. return reinterpret_cast&lt;uintptr_t&gt; (value);}bool InternedString::operator== (const InternedString &amp;_other) const{ // We can compare pointers directly, // because interned strings are never duplicated in memory. return value == _other.value;}bool InternedString::operator!= (const InternedString &amp;_other) const{ return !(*this == _other);}And that’s everything we need to do in order to implement string interning in a simple manner! You can already test whether it works correctly:InternedString first {\"Hello, world!\"};InternedString second {\"Welcome-welcome!\"};InternedString third {\"Hello, world!\"};std::cout &lt;&lt; \"first = \" &lt;&lt; *first &lt;&lt; std::endl;std::cout &lt;&lt; \"second = \" &lt;&lt; *second &lt;&lt; std::endl;std::cout &lt;&lt; \"third = \" &lt;&lt; *third &lt;&lt; std::endl &lt;&lt; std::endl;std::cout &lt;&lt; \"first == second: \" &lt;&lt; (first == second ? \"true\" : \"false\") &lt;&lt; std::endl;std::cout &lt;&lt; \"second == third : \" &lt;&lt; (second == third ? \"true\" : \"false\") &lt;&lt; std::endl;std::cout &lt;&lt; \"first == third : \" &lt;&lt; (first == third ? \"true\" : \"false\") &lt;&lt; std::endl;Expected output:first = Hello, world!second = Welcome-welcome!third = Hello, world!first == second: falsesecond == third : falsefirst == third : trueOf course, this implementation is trivial and ignores some important concepts like unused string deallocation and string-related memory defragmentation. We will explore these 2 topics below.Destiny of unused interned stringsIn our trivial implementation interned strings are never truly deallocated: they persist for whole program execution.Is it a real problem? Generally speaking, it is, but lots of implementations stick to no-deallocation solution forseveral reasons: Usually interned strings are well-known values like constants or some ids. In this case they are either always usedanyway or quickly become used again after short period of being unused. So there is no sense to deallocate them. Interned strings are usually quite small, therefore reference counters might significantly increase memory usage. Reference counting makes copy constructor, move constructor and assignments non-trivial: we can not just copypointer like we’re doing in no-deallocation solution. Performance impact is small, but working with non-trivial types is more difficult from the architectural point of view. If strings are ever deallocated, it’s not possible to use addresses as stable hashes anymore! If some genericstructure stores hashes without values, interned string might become unused and will be deallocated, leavingits address and therefore its hash value free to grab for new interned string, which would invalidate affected hash structure.Due to these reasons Emergence implementation of this conceptnever deallocates unused interned strings. Press Fire Games internal enginealso never deallocates unused interned strings. But Java uses garbage collector to deallocate interned strings. So it is up to you to decide whether you need to do something about unused interned strings or not.Fighting defragmentationStrings are small objects with wide variety of different sizes. When we’re allocating strings through global heapallocator like new or malloc at random moments, we’re creating perfect ground for memory defragmentation.Of course, it’s possible to solve this problem too!The core idea is to preallocate big memory blocks where interned strings will be stored and then use customallocator to allocate strings inside these blocks. If there is not enough memory for interning new string,additional memory block will be allocated. If we’re not deallocating unused interned strings, stack allocator isboth the simplest and the most efficient custom allocator for this purpose: each block has its own stack allocatorthat pushes new strings into itself. In this case we can call the whole structure for managing memory for internedstrings a pool of stacks: we’re using pool-based allocator to allocate new stack allocators that will allocatespace for interned strings. It might sound monstrous, but it is actually quite easy to implement and use.But what changes when we’re deallocating unused interned strings? Not a lot, actually! Sure, we cannot use stackallocators anymore, but we can use heap allocators instead. Therefore, the whole structure would become a pool of heaps. But beware of defragmentation inside interned string heaps: if strings are allocated anddeallocated often we might still encounter it inside these heaps.Instead of modyfying our simple implementation I will direct reader right into Emergence implementation of interned strings called UniqueString – there are header and object files. UniqueStrings are never deallocated and therefore use a pool of stacks approach.Hope you’ve enjoyed reading! :)" }, { "title": "Problem solving: Validating ECS graph", "url": "/posts/ProblemSolvingValidatingECSGraph/", "categories": "Tutorials, Algorithms", "tags": "Tutorials, C++, Algorithms, ECS", "date": "2022-07-06 15:15:00 +0300", "snippet": "Designing algorithms is quite rare task, so it is always feels great when you can apply this skill to solve somepractical problem. In this post I’ll talk about ECS graph validation algorithm that I’ve written forEmergence project.Problem definitionLet’s start from defining properties of ECS system: List of resources to which system has read-only access. List of resources to which system has write access. List of systems that are dependencies of this system.ECS graph is a set of systems where all references are internal. That means that if system A depends on system B andsystem A is a part of the graph than system B should be part of the graph too. It is called graph because it is usuallyvisualized as directed graph of systems where dependencies are edges: if system A depends on system B than thereis an edge from B system node to A system node. To make sure that graph is valid and can be executed we need to check that: There are no deadlocks caused by dependencies. There are no race conditions: when one system modifies resource other systems should not be able to access it. All the references in system lists (resources and dependencies) are valid.Checking that all references in system properties are valid is trivial, so we will ignore this validation step here.Applying theoryTo design an algorithm we firstly need to define what we’re doing mathematically. Let’s start from our ECS graph:it is directed graph \\(G (V, E)\\), where \\(V\\) is equal to the set of all systems in a graph. Then set of edges\\(E\\) is defined like that:\\[\\forall A, B \\in V \\hspace{1em} \\exists! (A, B) \\in E \\Leftrightarrow A \\in dependencies(B)\\]Now lets translate deadlock check into more math-friendly variant. If deadlock happens in ECS graph context it means that system is waiting for dependency that cannot be finished. In this context it can happen only if system dependson itself, because every system must be finishable by definition. Such dependency may arise only if \\(G\\) hasany cycle, so to pass this verification check \\(G\\) must be an acyclic graph.Race condition happens when one system reads or modifies resource, for example component storage, while other systemalso modifies this resource. System dependencies must be specified in a way that prevents such race conditions fromhappening. So, how do we check that there is no system accesses resource \\(R\\) while system \\(A\\) is modifying this resource? Concurrent access to one resource \\(R\\) can be prevented by dependencies in two ways:\\[\\forall A, B \\in V \\hspace{1em} \\exists path (A \\to B) \\rightarrow \\nexists RaceCondition\\]\\[\\forall A, B \\in V \\hspace{1em} \\exists path (B \\to A) \\rightarrow \\nexists RaceCondition\\]On top of that we can build race condition criteria:\\[\\forall A, B, R \\hspace{1em} modifies (A, R) \\land accesses (B, R) \\land \\nexists path (A \\to B) \\land \\nexists path (B \\to A) \\Leftrightarrow \\exists RaceCondition\\]I won’t bother readers with formal proof because it’s kind of boring. The main idea here is that absence of dependencies between any task that modifies \\(R\\) and any task that accesses \\(R\\) leads to a race condition.You can draw several graphs yourself to get a grip of this idea.SolutionWe’ve found out what we need to do mathematically, now it is time to implement it!Let’s start from cycle detection: I’ve decided to use depth-first search based graph traversal. Each node will be markedwith one of 3 markers: Unvisited, InStack or Verified, and all nodes will be marked as Unvisited from the start.It’s better to explain how this works through pseudocode:// This is our recursive visitor for DFS traversal.function VisitSystem (system){ // If node is alrady verified, we can be sure that // there is no cycles that involve this node. if (marks[system] == Verified) { return CycleNotFound; } // We've reached the node which children we're currently // iterating. It means that node can be reached from itself // and therefore is a part of the cycle. if (marks[system] == InStack) { return CycleFound; } // Mark that we're currently checking dependant systems // from this node. marks[system] = InStack; // Recursively visit all the dependant systems // (dependency -&gt; dependant edges). for (dependantSystem : GetDependantSystems (system)) { if (VisitSystem (dependantSystem) == CycleFound) { return CycleFound; } } // We've recursively visited all graph nodes that can be // reached from this node and found no cycles. Therefore, // we can say that this node is successfully verified. marks[system] = Verified; return CycleNotFound;}// This is our entrypoint to recursive VisitSystem-based traversal.function SearchForCycles (){ // To traverse the whole graph we need to visit every // system without dependencies. for (starterSystem : GetSystemsWithoutDependencies ()) { if (VisitSystem (starterSystem) == CycleFound) { return CycleFound; } } return CycleNotFound;}That’s all for our cycle detection, but what about race conditions? At first glance their detection looks much lessstraighforward. Of course, we could just bruteforce this and search path from every system, but it would be tooinefficient. Thankfully, we could modify our visitation algorithm from cycle detection check to collect all the reachable nodes for every node!function VisitSystem (system){ if (marks[system] == Verified) { return CycleNotFound; } if (marks[system] == InStack) { return CycleFound; } marks[system] = InStack; for (dependantSystem : GetDependantSystems (system)) { if (VisitSystem (dependantSystem) == CycleFound) { return CycleFound; } // Surprisingly, that's the whole change! // I won't strictly prove it here, because it // is obvious after drawing DFS of any directed // acyclic graph. reachable[system] += dependantSystem; reachable[system] += reachable[dependantSystem]; } marks[system] = Verified; return CycleNotFound;}Now \\(path (A \\to B)\\) check can be replaced with reachable[A].contains(B) check. After that everything is quitestraighforward: using our reachable map we can detect all possible race conditions as pairs of mutually unreachable nodes and check whether these nodes access or modify the same resource.OptimizationsPseudocode above omits some implementation details that can be crucial for algorithm performance. It is logical,because it is pseudocode after all. Therefore, I’ve decided to list these details below: Use numeric indices for systems and resources: it allows to make marks and reachable arrays insteadof maps, which will significantly speed up operations on them. Use bitsets for reachability recording: using bitsets instead of arrays to record reachable nodes would improveboth performance and memory usage. Cache dependant systems for every system: in initial format every system stores its dependencies instead ofdependant systems which makes GetDependantSystems function quite slow. You can reverse dependencies to getall dependant systems for all nodes in one iteration before doing graph traversal, which would significantlyimprove performance by making GetDependantSystems complexity O(1).Implementation in EmergenceAbove I’ve described the algorithm that I’ve used to verify ECS graph in Flow library for Emergence project. Of course, this implementation is a bit morecomplex and specific, because Flow task registration routineis more advanced than system registration in our example problem, but it still remains technically the samealgorithm.Hope you’ve enjoyed reading! :)" }, { "title": "Tutorial: Link-time polymorphism basics", "url": "/posts/TutorialLinkTimePolymorphismBasics/", "categories": "Tutorials, Architecture", "tags": "Tutorials, C++, CMake", "date": "2022-07-01 15:15:00 +0300", "snippet": "Everyone knows what runtime and compile time polymorphisms are, but what about link-time polymorphism? There is muchless materials about this type of polymorphism and it seems almost forgotten. So I’ve decided to write tutorial about its basics.What link-time polimorphism is?Runtime polymorphism operates on top of virtual methods, lambdas and function pointers. Compile time polymorphismutilizes special template-based techniques like SFINAE and CRTP to get things done. But what could we do during linktime? While assembling compiled object files into monolithic libraries and executables linker also resolves so-calledexternal references: declarations that are visible to compiled object but are defined outside of its scope. And that’sexactly where the secret of link-time polymorphism is hidden: we’re selecting where linker searches for these definitions and by switching definitions we could achieve polymorphism. The most classic example is C standard library:if you’re using only standard functions, your program could be compiled on any system that supports C, but each timedifferent implementation library would be linked to your program. Let’s discuss pros and cons of this method.Pros: There is no runtime cost of link-time polymorphism: call cost is equal to regular function call and even inliningis possible through link-time optimizations. There is no additional compile time cost: we’re not creating hordes of templates like in compile time polymorphism,therefore compilation time is not higher that for regular code without polymorphism. It’s the only method that can be used to hide platform-specific code.Cons: You cannot change implementation while program is running because linking is already done. However, if dynamiclinking is used, it is possible to change implementation between program executions by swapping shared library. You need buildsystem support: decision what libraries to link is done through build system, therefore you needto be ready to modify it.As you can see, link-time polymorphism is not a silver bullet and it has its own restrictions. But, in my opinion,it deserves to be used much more that it is used nowadays: in a lot of cases we don’t really need to change implementation while program is running, but runtime polymorphism is still used and so we’re paying excessiveperformance price for it.Example problemLet’s imagine that our application needs to serialize data in textual format in development mode for easier debuggingand in binary format in production mode for smaller file size. The most classic solution for such cases is runtimepolymorphism: using pure virtual class with two implementations: for production and for development. It is workingsolution, but is looks kind of clumpsy for me: We don’t ever need to switch implementations during runtime, but we’re paying runtime polymorphism costs. Unneded development logic will be compiled into production executable and vice versa. It’s not critical, of course,but not elegant either.Through runtime polymorphism works well enough here, I’m not satisified with this kind of approach, thereforeI’ve decided to write simple step-by-step tutorial how to do that with link-time polymorphism.Initial solutionLet’s start from the simplest way of implementing link-time polymorphism: object file selection. It means that we decideon buildsystem level which files belong to the target and by that we are selecting correct implementation for our target.InterfaceThe best way to start is to draft our serialization interface. For the purpose of simplicity we will just serializestrings and ints and append comments to development mode files. Of course, it is not even near the real serializers,but it’s better to have simple unrealistic example than difficult to understand but real one.#pragma once#include &lt;cstdint&gt;// We are using PImpl approach to hide implementation data from user code.// https://en.cppreference.com/w/cpp/language/pimpl// It also allows to easily switch DLLs.//// The other approach is to use inplace fixed size array, which is better for// this case, but makes tutorial much less readable. So I've decided to sacrifice// performance for readability, because performance means nothing here.struct SerializerImplementation;class Serializer final{public: explicit Serializer (const char *_outputFileName) noexcept; Serializer (const Serializer &amp;_other) = delete; Serializer (Serializer &amp;&amp;_other) noexcept; ~Serializer () noexcept; void WriteInt32 (int32_t _number) noexcept; void WriteAsciiString (const char *_string) noexcept; void WriteAsciiComment (const char *_string) noexcept;private: // We cannot use std::unique_ptr here because SerializerImplementation is undefined. SerializerImplementation *implementation = nullptr;};ImplementationsBoth our implementations will write data to a file through std::ofstream. So they will have same SerializerImplementation, move constructor and destructor. We don’t want to duplicate this code, thereforeit makes sense that this code should be common for both implementations.#pragma once#include &lt;fstream&gt;#include &lt;Serialization/Serializer.hpp&gt;struct SerializerImplementation{ std::ofstream output;};#include &lt;Serialization/Common/SerializerPrivate.hpp&gt;Serializer::Serializer (Serializer &amp;&amp;_other) noexcept : implementation (_other.implementation){ _other.implementation = nullptr;}Serializer::~Serializer () noexcept{ delete implementation;}Now lets focus on our sample implementations. Development implementation will save each element directly as text,separating them using line ends:#include &lt;Serialization/Common/SerializerPrivate.hpp&gt;#include &lt;Serialization/Serializer.hpp&gt;Serializer::Serializer (const char *_outputFileName) noexcept : implementation (new SerializerImplementation {std::ofstream {_outputFileName, std::ofstream::out}}){}void Serializer::WriteInt32 (int32_t _number) noexcept{ implementation-&gt;output &lt;&lt; _number &lt;&lt; std::endl;}void Serializer::WriteAsciiString (const char *_string) noexcept{ implementation-&gt;output &lt;&lt; _string &lt;&lt; std::endl;}void Serializer::WriteAsciiComment (const char *_string) noexcept{ implementation-&gt;output &lt;&lt; \"# \" &lt;&lt; _string &lt;&lt; std::endl;}For Production implementation we will directly use binary output:#define _CRT_SECURE_NO_WARNINGS#include &lt;Serialization/Common/SerializerPrivate.hpp&gt;#include &lt;Serialization/Serializer.hpp&gt;Serializer::Serializer (const char *_outputFileName) noexcept : implementation ( new SerializerImplementation {std::ofstream {_outputFileName, std::ofstream::out | std::ofstream::binary}}){}void Serializer::WriteInt32 (int32_t _number) noexcept{ // It is incorrect to save ints like this because we ignore endianness. But it is ok enough for our sample. implementation-&gt;output.write (reinterpret_cast&lt;const char *&gt; (&amp;_number), sizeof (_number));}void Serializer::WriteAsciiString (const char *_string) noexcept{ // We are adding 1 to strlen in order to capture zero terminator. implementation-&gt;output.write (_string, strlen (_string) + 1u);}void Serializer::WriteAsciiComment (const char *_string) noexcept{ // There is no comments in production files.}User applicationFor this sample we will keep user application as small as possible, because we have no need for complex logic here:#include &lt;Serialization/Serializer.hpp&gt;int main (){ Serializer serializer {\"Test.out\"}; serializer.WriteAsciiComment (\"This is test file comment line.\"); serializer.WriteAsciiString (\"Hello, world!\"); serializer.WriteInt32 (1024); return 0;}CMake scriptFinally, we have arrived at our final and most important logic: CMake build system script. It is fairly simplisticbecause we’re using the simpliest approach of link-time polymorphism:cmake_minimum_required (VERSION 3.21)project (LinkTimePolymorphismBasicsProject)set (CMAKE_CXX_STANDARD 20)# Build system option that is used for implementation selection.option (DEVELOPMENT \"Whether to use development serialization library.\" OFF)# Declare core set of sources, that are used in any case.set (SOURCES \"App/Main.cpp\" \"Library/Serialization/Serializer.hpp\" \"Library/Serialization/Common/SerializerPrivate.cpp\" \"Library/Serialization/Common/SerializerPrivate.hpp\")# Depending on build system option append source file with implementation.if (DEVELOPMENT) list (APPEND SOURCES \"Library/Serialization/Development/Serializer.cpp\")else () list (APPEND SOURCES \"Library/Serialization/Production/Serializer.cpp\")endif ()# Finally, create our application target.add_executable (App ${SOURCES})target_include_directories (App PRIVATE \"Library/\")Despite being the most simple and straighforward way to use link-time polymorphism, direct file selection is the mostwidely used approach: it is the usual go-to solution for platform-independence layers implementations and otherlayers, like graphics API independence layer. But it is not “poetic” enough, isn’t it? Below we will discover morescalable and more “poetic” approaches of link-time polymorphism.Implementations as separate librariesWhile direct file selection is simple, it is also not really scalable: it would be difficult to manage APIs with lotsof files like that. But there is more scalable approach: creating and linking diffirent CMake libraries. Let’s modifyour example to use this approach.To do this we need to split our CMake script into several ones. Let’s start from CMakeLists.txt for Serialization library:# Let's start from header-only API target. It only contains include path and our API header.add_library (SerializationAPI INTERFACE \"Serializer.hpp\")target_include_directories (SerializationAPI INTERFACE \"..\")# Declare common library target that will be used by both our implementations.add_library (SerializationCommon STATIC \"Common/SerializerPrivate.cpp\" \"Common/SerializerPrivate.hpp\")target_link_libraries (SerializationCommon PUBLIC SerializationAPI)# Declare Development implementation library.add_library (SerializationDevelopment STATIC \"Development/Serializer.cpp\")target_link_libraries (SerializationDevelopment PUBLIC SerializationCommon)# Declare Production implementation library.add_library (SerializationProduction STATIC \"Production/Serializer.cpp\")target_link_libraries (SerializationProduction PUBLIC SerializationCommon)Let’s also move our test executable setup into separate script:add_executable (App \"Main.cpp\")# Link required implementation library depending on build option.if (DEVELOPMENT) target_link_libraries (App PRIVATE SerializationDevelopment)else () target_link_libraries (App PRIVATE SerializationProduction)endif ()After that our root script will become quite small:cmake_minimum_required (VERSION 3.21)project (LinkTimePolymorphismBasicsProject)set (CMAKE_CXX_STANDARD 20)# Build system option that is used for implementation selection.option (DEVELOPMENT \"Whether to use development serialization library.\" OFF)# Just add our scripts for application and library.add_subdirectory (App)add_subdirectory (Library/Serialization)This approach is not only more advanced and scalable: it is also much more CMake-friendly, because changing source file list triggers full target recompilation, but changing link dependency only triggers relinking! It means that changinglink-time polymorphism implementation will be quite fast, because compiler won’t need to recompile any of thesource files.Switching implementations without rebuilding the applicationOne of the coolest moments about link-time polymorphism is that we can swap implementations between program executionsif we’re using dynamic linking. Let’s try it out! To migrate to dynamic linking we firsly need to add export/importinformation to our API (it is required only on Windows, but nevertheleess it’s better to know how to do it):// Includes ...// Declare whether we're exporting or importing dynamic symbols. Needed only for Windows builds.#ifdef SERIALIZATION_IMPLEMENTATION# define SERIALIZATION_API __declspec(dllexport)#else# define SERIALIZATION_API __declspec(dllimport)#endif// ...class SERIALIZATION_API Serializer final// ...Now we need to update our libraries build script by making several changes: Adding SERIALIZATION_IMPLEMENTATION compile define to targets that implement any methods. Making our libraries SHARED for dynamic linking. Making sure that both implementation libraries outputs are named Serialization so we can swap files.After these changes library script will look like this:# Let's start from header-only API target. It only contains include path and our API header.add_library (SerializationAPI INTERFACE \"Serializer.hpp\")target_include_directories (SerializationAPI INTERFACE \"..\")# Declare common library target that will be used by both our implementations.# NOTE: In order for implementation detection to work properly we need to make # this library shared too. Implementation detection will flag `no destructor`# link error if we link it as static library like before.add_library (SerializationCommon SHARED \"Common/SerializerPrivate.cpp\" \"Common/SerializerPrivate.hpp\")target_link_libraries (SerializationCommon PUBLIC SerializationAPI)target_compile_definitions (SerializationCommon PRIVATE SERIALIZATION_IMPLEMENTATION)# Declare Development implementation library.add_library (SerializationDevelopment SHARED \"Development/Serializer.cpp\")target_link_libraries (SerializationDevelopment PUBLIC SerializationCommon)target_compile_definitions (SerializationDevelopment PRIVATE SERIALIZATION_IMPLEMENTATION)set_target_properties (SerializationDevelopment PROPERTIES OUTPUT_NAME \"Development/Serialization\")# Declare Production implementation library.add_library (SerializationProduction SHARED \"Production/Serializer.cpp\")target_link_libraries (SerializationProduction PUBLIC SerializationCommon)target_compile_definitions (SerializationProduction PRIVATE SERIALIZATION_IMPLEMENTATION)set_target_properties (SerializationProduction PROPERTIES OUTPUT_NAME \"Production/Serialization\")As the last step we need to teach our example app target to copy libraries to its folder:# Target definition...# We need to copy runtime libraries to our executable folder.add_custom_command ( TARGET App POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_if_different $&lt;TARGET_RUNTIME_DLLS:App&gt; $&lt;TARGET_FILE_DIR:App&gt; COMMAND_EXPAND_LISTS)And that’s all that we need to do in order to make implementations swappable as files! Now you can try to replaceDLL (or SO) files and see that it really works! You can download the full source code from GitHub repository.Hope you’ve enjoyed reading! :)" }, { "title": "Emergence: Interpolating transform", "url": "/posts/EmergenceInterpolatingTransform/", "categories": "Emergence, Development Log", "tags": "Emergence, GameDev, Math", "date": "2022-06-27 20:30:00 +0300", "snippet": "What if I say that every game object in Emergence has two transforms? What for? For the sake of making everythinglook cool, of course! In this post I will describe transform interpolation, which is quite useful feature unless you’re entirely sure that variable framerate is the best option for you.Framerate and world updatesThere is a perfect article about time steps by Glenn Fiedler.But just sending you to the other blog wouldn’t be polite, so I’ll write my view on this topic too.By far the simpliest approach to update game world is to execute it before every render and passing frame delta timeas time step. Despite being primitive, it is widely adopted and even used in Unreal Engine! But there is a onenot-so-simple problem with this approach: it makes your simulation non-deterministic, because time step is equal toframe time delta and therefore is hardware-dependent. And this problem triggers a bunch of smaller problems:physics simulation might be buggy and strange, multiplayer networking is difficult because of non-determinism,subtle and hard to figure out gameplay bugs (or features! :) ) may arise.The other option is fixed+normal updates approach: physics simulation, networking and “strong” gameplay logic are updated with fixed time steps using time accumulation, while render and visual logic are updated using variable framerate technique from previous paragraph. This allows to both have deterministic simulation of features that workin fixed update, and responsive visual that works in normal update. It might sound like a miraculous solution,but there is a reason why it is not as widely adopted as simple variable framerate: it is more difficult towork with two separate update pipelines than with one. For example, it creates problem of transform interpolation:transforms are updated in fixed pipeline, but we would like to see smooth movement instead of jittery transformchanges. And it is exactly the theme of this post!For those who have read Glenn’s article, I would like to highlight that Celeritytreats time accumulation in a bit different way. In Glenn’s version normal update “produces” time and fixed update“consumes” time, but in my library I’ve decided to take inversed approach: fixed update “produces” time and normalupdate “consumes” it. The reason for this is that real time is always way ahead of render time if article version is used:----F----F--R-F--T-|----|----F -- executed fixed updates.R -- render time.T -- real time.| -- planned fixed update.After inversion fixed updates are ahead of real time, but render time is equal to real time:----F----F----F--T-F----|----Nevertheless, from perspective of input both variants have 1 frame latency, therefore difference is not so big and you’re free to use any variant.No interpolation vs InterpolationLet’s imagine that we directly use transforms from fixed update to render our objects. At 60 FPS everything probably looks superb. But then designers introduce slowdown feature, that allows player to run game at x0.25 speed forseveral seconds. Suddenly, during slowdown everything starts looking jittery:But we don’t want player to think that game lags during slowdown! We want something smoother, like this:To achieve smoothier picture we need to adapt to the situation and use different transforms for render.And that is exactly how visual transform interpolation works: we adjust render transforms and everything looks like miracle!Interpolation use casesSome rare slowdown feature is not the only case where you need interpolation. There are many more other cases, for example: Sometimes users might like to watch the game at much higher framerate that we could possibly achieve by doing1 normal update per 1 fixed update. I don’t really see the difference between 60 and 120 FPS, but some people do.For these people we need to render several frames per fixed update and interpolate transforms. Sometimes fixed update might drastically slow down game performance, for example when doing difficult dynamicscalculations. It might even result in death spiral: it is when we are doing more and more fixed updateseach frame because frame time is bigger than fixed step time. To solve this issue we need to temporarilyincrease fixed time step, for example from 60 FPS to 15 FPS (yeah, technically we’re increasing it, because time step value increases), and to keep visual effects clean while doing so we need to interpolate. Multiplayer games usually have low replication frequency to optimize traffic usage. For example, Apex Legendssends replication data at 20 samples per second. Of course, it would have looked laggy without interpolation! Interpolation in CelerityNow it’s finally time to tell how I implemented interpolation in Celerity!Let’s start from the concept of interpolated movement: we need to enable interpolation only when object actually moves,otherwise we will not only spend CPU time on useless work, but could also see incorrect results: for example, we can interpolate teleportation or interpolate spawn by moving object from world zero to spawn transform. To achieve thatwe need to use special flag inside Transform3dComponent that I called visualTransformSyncNeeded. This flagis true only when object is in interpolated movement mode and always false otherwise. For spawns and teleports there isspecial a _skipInterpolation parameter in transform setter, that allows to move objects without enablingvisualTransformSyncNeeded. That’s how logical transform setter code looks like:void Transform3dComponent::SetLogicalLocalTransform (const Math::Transform3d &amp;_transform, bool _skipInterpolation) noexcept{ // ... Irrelevant setter logic ... if (_skipInterpolation) { // We do not enable interpolation, therefore we need to set visual transform directly. SetVisualLocalTransform (logicalLocalTransform); // If object was in interpoolated movement mode -- interrupt it. visualTransformSyncNeeded = false; } else { // If we're not in interpolated movement mode, we need to reset sync timer // to correctly process pauses between two different interpolated movements. // Otherwise, interpolation result would be incorrect. // More about that timer below. if (!visualTransformSyncNeeded) { visualTransformLastSyncTimeNs = 0u; visualTransformSyncNeeded = true; } }}To apply interpolation we need to know 2 time stamps: Last fixed timestamp when logical transform was changed. Last normal time stamp during which visual transform had desired value. Getting first one is pretty easy: we need to save it as variable inside transform and update it during interpolatedmovement whenever we detect that transform was changed, like this:// Celerity uses revision-driven transform change detection instead of dirty flags, // because it allows to avoid going through full hierarchy when transform is setted.if (transform-&gt;lastObservedLogicalTransformRevision != transform-&gt;logicalLocalTransformRevision){ transform-&gt;lastObservedLogicalTransformRevision = transform-&gt;logicalLocalTransformRevision; // We've detected transform change and now can save required timestamp. transform-&gt;logicalTransformLastObservationTimeNs = time-&gt;fixedTimeNs;}Getting second one is more difficult at first glance, but actually it is easy too. After each interpolation operationvisual transform has desired value, therefore during interpolated movement second timestamp is always equal to thenormal time of previous frame. But what about first frame of interpolated movement? Because movement has just started,object is already in desired transform, therefore time stamp is equal to frame normal time. Code looks likethis:// Interpolated movement detected. Remember setter code above? // Yep, that's why we set it to zero there.if (transform-&gt;visualTransformLastSyncTimeNs == 0u){ transform-&gt;visualTransformLastSyncTimeNs = time-&gt;normalTimeNs;}// ... Actual interpolation execution ...transform-&gt;visualTransformLastSyncTimeNs = time-&gt;normalTimeNs;This two time stamps are our interpolation borders and current normal time is point at which we need to calculate transform. Now we have enough information to do transform linear interpolation:// Time elapsed since last disered visual transform.const uint64_t elapsed = time-&gt;normalTimeNs - transform-&gt;visualTransformLastSyncTimeNs;// Known interpolation interval duration.const uint64_t duration = transform-&gt;logicalTransformLastObservationTimeNs - transform-&gt;visualTransformLastSyncTimeNs;const float progress = Math::Clamp ( static_cast&lt;float&gt; (elapsed) / static_cast&lt;float&gt; (duration), 0.0f, 1.0f);// Note, that we are only working with local transforms here: // we don't need to take global ones into account because// we're processing every transform that needs interpolation,// so if parent transform was changed, it would be processed separately.const Math::Transform3d &amp;source = transform-&gt;GetVisualLocalTransform ();const Math::Transform3d &amp;target = transform-&gt;GetLogicalLocalTransform ();transform-&gt;SetVisualLocalTransform ({Math::Lerp (source.translation, target.translation, progress), Math::SLerp (source.rotation, target.rotation, progress), Math::Lerp (source.scale, target.scale, progress)});Simple and straighforward, isn’t it? There is only one last detail left: we need to exit from interpolated movement mode after arriving at target destination. It is easier than it sounds:transform-&gt;visualTransformSyncNeeded = transform-&gt;visualTransformLastSyncTimeNs &lt; transform-&gt;logicalTransformLastObservationTimeNs;And that’s all! Transform interpolation might sound scary, but it is quite simple and minimalistic once you get itdone right. You could check resulting code files here: Transform3dComponent header file. Transform3dComponent object file. Transform3dVisualSynchronizer task implementation.Hope you’ve enjoyed reading! :)" }, { "title": "Emergence: Why is there no entities in Celerity?", "url": "/posts/EmergenceWhyIsThereNoEntitiesInCelerity/", "categories": "Emergence, Development Log", "tags": "Emergence, ECS, GameDev", "date": "2022-06-23 12:40:00 +0300", "snippet": "Entity is the first of the three core principles of ECS design pattern. Nevertheless, I’ve decided that there should beno entities in Emergence ECS-like framework Celerity.I will tell you how I’ve arrived at this decision in this post.ECS patternFor those who are not familiar with the Entity-Component-System pattern I have decided to write a quick recap.Historically, ECS is the evolution of the Components pattern,in which the parts of the game world, usually called game objects or actors, can receive additional features throughthe addition of components with custom logic. This pattern has saved quite a few projects from the drawbacks of the malicious“just write the game logic and use inheritance” pattern, used extensively in the old days. That’s the reason whyComponents is a core feature of both well-known engines, likeUnity and Unreal Engine, and less known ones, like Urho3D.But Components pattern didn’t solve all the pitfalls of itspredecessor, and one of the most malicious problems was the update order. When every object and every componentchaotically subscribes to update the routine, it is not only difficult to know whether it uses the current frame or the previousframe state of the other objects, but also nearly impossible to track what happens during the game loop. This kind of chaosis a problem by itself, but it leads to another problem too: how to make the gameplay logic parallel when everything isso chaotic and uncontrollable?That’s where Entity-Component-System comes to the rescue by removing all the logic from the components and returningto the times of procedural programming. Sounds too dramatic and old-fashioned? Maybe, but it has really helped tosort things out, so let’s describe what it is by first describing 3 core pillars of ECS: Entity: it is something that exists inside the game world. It might be a tree, a player, aninvisible controller. But the entity by itself has no logic at all, it is just a collection of components! Nothing more,nothing less. Component: it is just a collection of data, possibly even a plain-old-data structure. Ideally, components haveno logic at all, just a data layout. But sometimes there are utility functions that help access data or modify itwithout breaking the coherency. System: it is a place to put the game logic into. It might be a function or something more sophisticated. The corerule is that all meaningful logic should be placed inside systems, and the systems themselves depend on each other to makethe update order predictable. So, what problems does this pattern solve? Update order is always predictable due to the dependencies between the systems that contain all the logic. It is much easier to make the logic parallel, because systems access well-known sets of components, so we canjust execute them simultaneously if they do not both modify the same component type. It is much easier to serialize data and send it over the network, because the logic is separated from the data.Now you know what ECS theoretically is, so we can discuss the topic of entities deeper.What is an Entity?Many old-school developers still think of entities as game objects without logic: just the game objects that storemultiple components. It is not incorrect to think like that, but in my opinion it hides the storage-managementaspect of ECS by simply treating entities as vectors.Modern approaches usually treat entities as just numbers. Yes, simple numbers that are used to query componentsfrom their component storages. For example, we can have a separate memory pool for each component type, so that the accessto several components of the same type will be much more cache coherent. Of course, it is not the only “correct”way to treat the entity concept: there are things like entity family locality optimizations, but I won’t dig furtherinto that topic in this post as I’m not an expert in that.But let’s dig deeper into the “entity as number” idea. Doesn’t it remind you of some other topics from computerscience? Like foreign indices from relational databases? The thing is that we can view component types as tablesand entities as foreign keys that create relations between components. I like to draw parallels between differentcomputer science topics, and therefore I really like this connection: it looks like a powerful path to improveECS pattern even further by adding an ability to create other indices instead of only querying by the entity id.It is not a fresh idea, but I’ve never seen any successful implementations of that principle, therefore I have decidedthat I must try to implement it myself.How the additional indexing helps?At first glance additional indices might sound excessive. Why do we need them after all if everything worked finewithout them? But I’m convinced that there are enough cases to justify that we need them: In some projects there are a lot of so-called flag components, like AliveComponent, ReadyToSpawnComponent, etc.These components have no data and are only used to filter entities. But if we were able to create additional indices,we wouldn’t need these components anymore: we could just store this data as fields, like bool alive, and createindices for these fields. Feel the need to iterate over all the alive units? Just use a query on this index. I’ve seen a lot of expressions like if (timeNow &lt; component-&gt;coolingDownUntil) continue;. It’s not only bad forthe performance, but also decreases readability by adding checks like this everywhere. But why not create a sorted indexon coolingDownUntil? It would allow us to get the cooled down entities fast and without the iterate-over-everythingoverhead. And because the cool down is not really applied every frame, the index usage overhead will be really tiny. Almost always there are some so-called managers that accompany the game world. For example, ConfigManager orWeaponStatsManager. Usually, these managers work as glorified hash maps, which made me think: why not puttheir data as additional tables inside the game world? We can use the same API to create indices over configs and statsids and to get rid of excessive managers. Game world as a universal storageBy allowing users to create additional tables and indices for whatever they need, we convert the game world into somekind of universal storage. We can store a lot of things there: Components, first and foremost. Records with a short lifetime, like events. Just don’t index them. :) Singletons. Configs. Stats. Maybe even assets!It might sound messy, but it also sounds really powerful, isn’t it? At least it is for me, therefore I decided tomake it the core idea for my ECS implementation. Sticking to this idea makes the principle of entities obsolete:we just have tables with records and systems (personally, I prefer to call them tasks) that operate on these records.Of course, this approach has its pros and cons. Let’s start from pros: User can store everything in the game world and treat all the data in the same way. Indices can be created for whatever user needs without restrictions.And there are cons: Entity family optimizations can not be applied here, because there are no entities. It might be difficult to navigate in storage, because it will store almost everything.For me, pros outweigh the cons, but you’re free to argue about it.In the end, I’d like to share some examples from my unfinished demo to show how this works.Inserting physical material into the world// Tasks (systems) store prepared queries as fields. This query allows user to insert records.Emergence::Celerity::InsertLongTermQuery insertDynamicsMaterial;// .../// Task initialize queries in constructors.: insertDynamicsMaterial (INSERT_LONG_TERM (Emergence::Physics::DynamicsMaterial)),// ...// Then we can insert materials using this query during task execution.auto materialCursor = insertDynamicsMaterial.Execute ();auto *material = static_cast&lt;Emergence::Physics::DynamicsMaterial *&gt; (++materialCursor);material-&gt;id = PhysicsConstant::DEFAULT_MATERIAL_ID;material-&gt;staticFriction = 0.4f;material-&gt;dynamicFriction = 0.4f;material-&gt;enableFriction = true;material-&gt;restitution = 0.3f;material-&gt;density = 400.0f;Querying by an entity (object) id// This query allows user to gain read-only access to all records with// requested value in given field.Emergence::Celerity::FetchValueQuery fetchShapeByObjectId;// ...: fetchShapeByObjectId (FETCH_VALUE_1F (CollisionShapeComponent, objectId)),// ...// Process all shapes on object using prepared query and cycle.for (auto shapeCursor = fetchShapeByObjectId.Execute (&amp;objectId); const auto *shape = static_cast&lt;const CollisionShapeComponent *&gt; (*shapeCursor); ++shapeCursor){ // ...}Querying cooled down components// This query allows user to gain full write access to all records// where field value is inside given interval.Emergence::Celerity::ModifyAscendingRangeQuery modifyShootersByCoolingDownUntil;// ...: modifyShootersByCoolingDownUntil (MODIFY_ASCENDING_RANGE (ShooterComponent, coolingDownUntilNs)),/// ...// Process all the shooters where the cool down ended before this moment.// `nullptr` as query argument represents negative (if first) or positive (if second) infinity.for (auto shooterCursor = modifyShootersByCoolingDownUntil.Execute (nullptr, &amp;time-&gt;fixedTimeNs); auto *shooter = static_cast&lt;ShooterComponent *&gt; (*shooterCursor);){ // ...}That’s all for now. Hope you’ve enjoyed reading! :)" }, { "title": "Arrow to the Knee: Editing unordered_multiset items", "url": "/posts/ArrowToTheKneeEditingUnorderedMultisetItems/", "categories": "Notes, Arrow to the Knee", "tags": "Arrow to the Knee, C++, Containers", "date": "2022-06-20 12:20:00 +0300", "snippet": "Have you ever thought of editing std::unordered_multiset items while iterating over it? Does it even make sense?I will tell you why and how I tried to do it and what happened to my code afterwards in this post.Getting into positionstd::unordered_multiset iterator returns const references to items, and it works like that for a reason: developerswould’ve never expected that some psycho tried to edit items during iteration. And even if they did expect it,there is no practical way to make this work, because there is no cheap way to teach std::unordered_multisetto track changes in its records.So, why did I need to edit items, after all? This case arose duringPegasusdevelopment: user requests write access to the record, so we need to provide it as fast as possible, and only afteruser has done everything needed to be done do we check whether the key fields are different. Of course, we also have aseparate preallocated record that stores only the key fields to perform this check. So, when we find out that userhas changed the key fields, we need to somehow update std::unordered_multiset according to these changes.There is one more case which is even more complicated at the first glance: user might just delete the record. In thiscase we don’t want to check whether the key fields have been changed, we want to just erase this record right away.Getting shotThe first thought that came to my mind was: Oh, we already have iterators, so we can just erase and then emplace and everything will be fine!Sounds reasonable, right? Even if the value is not the same as it was before, we still have a valid iterator,so any operations on it should work fine. But it isn’t the case with unordered containers.The thing is, just having an iterator is not enough to perform the removal properly: unorderedcontainer needs to calculate the hash once more and that is where things are getting fucked up, because the hash isdifferent now.To check that, try running this snippet using any compiler and any STD implementation you like:std::unordered_multiset &lt;int&gt; set;set.emplace (1);set.emplace (2);set.emplace (3);auto iterator = set.find (2);const_cast &lt;int&amp;&gt; (*iterator) = 4;set.erase (iterator);for (const int &amp;number : set){ std::cout &lt;&lt; number &lt;&lt; std::endl;}You can also try to execute it with numbers other than 4: it would result in unexpected(or sometimes normal but not really expected) behavior.Healing the woundsBut inPegasuseverything works fine, so there is the solution for this problem, right? Yep, there is! A bit clunky and ad hoc though.We can start from the fact that we need to somehow keep the hash the same after changing key fields. That means, thatwe need to roll the key fields back to their initial values, right? This is the only way to think about that. And it is closeto howPegasusactually deals with this problem: it stores a multiset of pointers and already has a preallocated record with initialvalues for change checking, therefore we can just edit the iterator like that:// Copy link to the record for further usage.const void *record = *_position;// Replace iterator content with backup to make a hash result valid.const_cast&lt;void const *&amp;&gt; (*_position) = storage-&gt;GetEditedRecordBackup ();// Now we can safely erase record from our set.records.erase (_position);// And reinsert our changed record!records.emplace_back (record);Code above is a simplified version of whatPegasusdoes.But what if you don’t have a record backup? Well, if you know the index count during compile time, which is not the case forPegasus,there is another way to deal with this problem: storing the hash as an additional field and recalculating it only afterthe record is erased from the map. Unfortunately, I’ve never tried it myself, so I can’t tell you more about it.Hope you’ve enjoyed reading about this tragic failings with std::unordered_multiset! :)" }, { "title": "Hint: Global Initialization Order", "url": "/posts/HintGlobalInitializationOrder/", "categories": "Notes, Hints", "tags": "Notes, C++", "date": "2022-06-18 19:30:00 +0300", "snippet": "Global initialization order is a rather trivial thing, but anyone encounters mistakes related to it some day or another.I’ve decided to post the hint that I would’ve given to my past self. Let’s start from the problem definition.ProblemLet’s imagine that we have these two static fields:class MyHolderClass final{ static AClass aObject; static BClass bObject;};It doesn’t matter whether they are inside one holder class, two different ones or are top level citizens.There is one possible problem: aObject initializer might access bObject, but bObject is not yet initialized,or vice versa. Therefore, access to uninitialized memory occurs and no one can predict what happens then:behaviour varies from blunt crashes to subtle bugs.I guess, everyone sooner or later encounters bugs related to this problem. And everyone who did knowsthat it is a pain in the ass to debug them. So, how to avoid this problem?SolutionThankfully, there is such thing as function-scope static variables, that are initialized during the first call to thefunction. Of course, they have a small impact on performance (close to one additional if per call, I guess),but they allow us to forget about initialization order problems once and for all.// Header file.class MyHolderClass final{ static AClass &amp;GetA (); static BClass &amp;GetB ();};// Object file.AClass &amp;MyHolderClass::GetA (){ static AClass aObject; return aObject;}BClass &amp;MyHolderClass::GetB (){ static BClass bObject; return bObject;}In this case, the function call always guarantees that aObject and bObject are initialized by the time they areaccessed. And if their initializers depend on one another, we will receive easily debuggable stackoverflow crash.When this solution is bad If you’re completely sure that static global variables are never accessed during global initialization, it’s betterto avoid packing them inside functions, because this way you might reduce readability and introduce additionalperformance cost to variable access. If your hot path code accesses this global variable and needs to achieve top-notch performance, it is unwiseto add one unnecessary if to the equation (and possibly add one function call too, if LTOs are not smart enough). So choose wisely! :)This post might sound really trivial, but you shouldn’t just ignore this topic. I’ve encountered mistakes likethis in some codebases (hello Vizor Games mobile department!) and it wasn’t very pleasant." }, { "title": "Emergence: What & Why?", "url": "/posts/EmergenceWhat&Why/", "categories": "Emergence, Development Log", "tags": "Emergence, C++, GameDev", "date": "2022-06-18 17:00:00 +0300", "snippet": "I’ve been developing Emergence project for more than a year.Or two years, if you count a previous unfortunate attempt!I’ve decided to finally come out of the shadow and start writing about it.Today I am going to talk on the decision to start this project.This is my first post on this blog, therefore I think it’s a good idea to share what kinds of posts I plan to do later: Emergence Development Log: Everything about Emergence projectdevelopment. Emergence tutorials: Tutorials on how to work with Emergencelibraries once they’re ready, of course. :) Hints: Small notes about different aspects of programming. Arrow to the Knee: name speaks for itself, I guess. These notes will be about shooting yourself with your code. There might be articles and tutorials on general topics, connected to game development, but I’m not sure yet.If you’d like to get notifications about new posts, subscribe to myTelegram channel. :)MotivationI discovered data-driven programming when I joined BattlePrime team.These guys were really cool professionals, and I’m glad I was working with them.After years of object-oriented development I was really excited to try something new and quickly became a true ECS zealot.But there were a lot of flaws in our ECS design too, not because people were unprofessional, but because designingarchitecture is generally difficult.From my perspective, so called flag-components were one of the main problems in our design.We had AliveComponent, DeadComponent, ActiveAvatarComponent, ControlledByPlayerComponent and so on.These components had no data and were only used to filter entities through components masks.I was thinking about this a lot and one question was arising on and on: why can’t we just use query-like API instead ofclassic ECS entity families?I was digging further and further into this why not queries? question and started believing in database-drivenECS concept.Yes, it’s not new and many people before me tried to implement it and failed.But this idea was still very interesting and motivating for me.I had a lot of questions.What is the difference between receiving a component from entity and quering by entity id?Why do we even need entities, when everything can be records, loosely connected by indexed queries?So, I decided to write my own ECS framework with database-like style and prostitutes queries.Also, I was unsatisfied with some concepts that were highly used in BattlePrime andsome other ECS frameworks. Therefore, I decided to stick to some additional rules while writingEmergence: There should be no virtual methods unless they are required by third party libraries. Not only are virtualskind of slow from a performance point of view, but also a source of unknown behaviour. By allowing to put almosteverything into everything, virtuals might lead to strange bugs and unnecessary complications. I decided that Iwant everything to be straightforward, so there should be no virtuals. Template usage should be minimized. There are some people out there who really really love templates, but usuallythey ignore one significant drawback: compile time. One engineer in BattlePrime teamcalculated that including &lt;core/entity.h&gt; increases unit compile time by roughly 6 seconds. And &lt;core/entity.h&gt;was, obviously, included everywhere. In my opinion, long compile times make programming unbearable, therefore Idecided to limit template usage in order to have a reasonable compile time. Multithreaded code must be planned to be multithreaded from the beginning, not ad-hooked to be multithreaded after.One of the main advantages of data-driven design is that it is usually much easier to adapt it for parallel executioncomparing to a classic OOP approach. Unfortunately, BattlePrime team was never ableto make their ECS truly parallel: they tried several times, but failed. One of the biggest problems was that theirECS had never been designed to be multithreaded in the first place, so I decided to think about multithreadingfrom the start. There was also one additional thing I wanted to try out duringEmergence development: link time polymorphism. It sounded likesomething unbelievable: polymorphism without runtime performance and compile time drawbacks.GoalsAfter failing my previous attempt – Temple – I’ve decided that I need to understand better what I am writing beforestarting again from scratch.I was planning to create several disconnected or loosely connected modules and build my own ECS on top of them.And that’s exactly why I called this project Emergence: my ECS framework would emerge on top of different disconnectedlibraries, like consciousness on top of our neurons in emergent consciousness philosophy. Below I am going to providethe list of the libraries I’ve planned: CMake framework for link time polymorphism. I’ve planned many libraries and needed good support from the buildsystem for my beloved link time polymorphism. :) Memory allocation library with pools, stacks, unique strings and hierarchical memory profiling. While the first onesare quite easy to implement, hierarchical (aka “build memory usage flame graph for me, bitch”) memory profilinghas been quite tricky, because I haven’t been able to find any opensource tools that can do that. Simplistic field-only reflection that also knows about constructors and destructors. I needed reflection forcomponent indexing, and I didn’t want to use any of the existing heavyweight libraries, therefore I decided to createmy own lightweight library. Logging library, possibly a wrapper for some logging framework. How can you track bugs without logging? Utility for rendering complex runtime structures into DOT graphs. It is always useful to render your system graph orsome other data. Indexed object storage. Just a simple collection of objects with one no-so-simple feature: it must provide userwith an opportunity to create hash indices, sorted indices and volumetric (aka bounding box) indices. Library for managing indexed object storages that also supports unindexed storages and singletons. Sounds exactlylike ECS World, isn’t it? Library for registering and executing parallel tasks that use shared resources and depend on each other. Also,this library needs to detect circular dependencies and race conditions. Later it is going to be executing systems insidemy ECS framework. And finally, the pinnacle of the human development, my ECS framework. :D What is it now?Almost a year and three months have passed since I started developingEmergence, and I’m happy to say that almost all the goals abovehave been achieved. There is just one thing left to be done: I need to develop a small proof-of-concept project to checkwhether my ECS framework is good at doing what it needs to do. But, as it always happens, this last thing ismuch more complex than it sounds: to finish this project, I need to at least integrate physics and graphics. AndI’ve already finished small PhysX and Urho3D integrations, but who knows what other integrations I might need later.I hope to finish proof-of-concept in the next month or two and finally be able to present my beloved ECS frameworkto the public. Let’s just hope that there won’t be any complex obstacles in the process of doing so.That’s all for now. Hope you’ve enjoyed reading this article. :)" } ]
